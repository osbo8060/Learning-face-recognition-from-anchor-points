{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "#                   Entire File Made by Oscar Boman\n",
    "# ==============================================================\n",
    "\n",
    "import utils as u\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.spatial import Delaunay\n",
    "import trimesh\n",
    "from scipy.spatial import distance_matrix\n",
    "import os\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RECOGNITION_PATH = './dataset/anchor_points_dataset/face_recognition/'\n",
    "VALIDATION_PATH = './dataset/anchor_points_dataset/face_validation/'\n",
    "if os.path.exists(RECOGNITION_PATH):\n",
    "    shutil.rmtree(RECOGNITION_PATH)\n",
    "os.makedirs(RECOGNITION_PATH)\n",
    "\n",
    "if os.path.exists(VALIDATION_PATH):\n",
    "    shutil.rmtree(VALIDATION_PATH)\n",
    "os.makedirs(VALIDATION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'dataset\\anchor_points_dataset\\face_recognition'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 72\u001b[0m\n\u001b[0;32m     67\u001b[0m     df_pairwise\u001b[38;5;241m.\u001b[39mloc[index] \u001b[38;5;241m=\u001b[39m pairwise\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;241m+\u001b[39m [label]\n\u001b[0;32m     68\u001b[0m     df_shape_descriptors\u001b[38;5;241m.\u001b[39mloc[index] \u001b[38;5;241m=\u001b[39m centroid\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;241m+\u001b[39m gaussian\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;241m+\u001b[39m pairwise\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;241m+\u001b[39m [label]\n\u001b[1;32m---> 72\u001b[0m df_scaled_rotated\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./dataset/anchor_points_dataset/face_recognition/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrotated_data_points.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     73\u001b[0m df_fft\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./dataset/anchor_points_dataset/face_recognition/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfft_data_points.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     74\u001b[0m df_gaussian\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./dataset/anchor_points_dataset/face_recognition/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgaussian_data_points.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\46737\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3902\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3891\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3893\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3894\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3895\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3899\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3900\u001b[0m )\n\u001b[1;32m-> 3902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m   3903\u001b[0m     path_or_buf,\n\u001b[0;32m   3904\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   3905\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   3906\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   3907\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   3908\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3909\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m   3910\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   3911\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m   3912\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   3913\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   3914\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[0;32m   3915\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   3916\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[0;32m   3917\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[0;32m   3918\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3919\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\46737\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1152\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1134\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1135\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1151\u001b[0m )\n\u001b[1;32m-> 1152\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1155\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\46737\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:247\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    250\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    251\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,\n\u001b[0;32m    252\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression,\n\u001b[0;32m    253\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m    254\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    257\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    258\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    264\u001b[0m     )\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\46737\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:739\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[1;32m--> 739\u001b[0m     check_parent_directory(\u001b[38;5;28mstr\u001b[39m(handle))\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    743\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\46737\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:604\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    602\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m--> 604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'dataset\\anchor_points_dataset\\face_recognition'"
     ]
    }
   ],
   "source": [
    "# Reconstruct data\n",
    "data = pd.read_csv('./dataset/anchor_points_dataset/data_points.csv')\n",
    "def reconstruct_data(data):\n",
    "    data_tensor = torch.tensor(data[0:204])\n",
    "    data_restructured = torch.zeros(size=(3,68)) \n",
    "    data_restructured[0, :] = data_tensor[0:68]\n",
    "    data_restructured[1, :] = data_tensor[68:136]\n",
    "    data_restructured[2, :] = data_tensor[136:204]\n",
    "    return data_restructured\n",
    "\n",
    "def calculate_centroid(data):\n",
    "    sum_vectors = torch.sum(data, dim=0)\n",
    "    centroid = sum_vectors / data.shape[0]\n",
    "    return centroid\n",
    "\n",
    "def calculate_gaussian_curvature(data):\n",
    "    tri = Delaunay(data[:, :2])  \n",
    "    faces = tri.simplices\n",
    "\n",
    "    mesh = trimesh.Trimesh(vertices=data, faces=faces)\n",
    "    gaussian_curvature = trimesh.curvature.discrete_gaussian_curvature_measure(mesh=mesh, points=data, radius=1)\n",
    "    return gaussian_curvature\n",
    "\n",
    "def calculate_pairwise_point_distances(data):\n",
    "    return distance_matrix(data, data)\n",
    "\n",
    "columns = [f'feature_{i}' for i in range(1, 205)] + ['label']\n",
    "df_scaled_rotated = pd.DataFrame(columns=columns)\n",
    "\n",
    "columns = [f'feature_{i}' for i in range(1, 202)] + ['label']\n",
    "df_fft = pd.DataFrame(columns=columns)\n",
    "\n",
    "columns = [f'feature_{i}' for i in range(1, 72)] + ['label']\n",
    "df_gaussian = pd.DataFrame(columns=columns)\n",
    "\n",
    "columns = [f'feature_{i}' for i in range(1, 4625)] + ['label']\n",
    "df_pairwise = pd.DataFrame(columns=columns)\n",
    "\n",
    "columns = [f'feature_{i}' for i in range(1, 4625 + 71)] + ['label']\n",
    "df_shape_descriptors = pd.DataFrame(columns=columns)\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    row_list = row.values.flatten().tolist()\n",
    "    label = row_list[-1]\n",
    "    points = reconstruct_data(row_list)\n",
    "\n",
    "    points = u.min_max_scale_data(u.rotate_face(points))\n",
    "\n",
    "\n",
    "    # fft\n",
    "    fft_vector = u.fft_feature_vector(points)\n",
    "    \n",
    "\n",
    "    # centroid\n",
    "    centroid = calculate_centroid(points)\n",
    "\n",
    "    # gaussian\n",
    "    gaussian = calculate_gaussian_curvature(points)\n",
    "\n",
    "    # pairwise\n",
    "    pairwise = calculate_pairwise_point_distances(points)\n",
    "\n",
    "    df_scaled_rotated.loc[index] = points.reshape(-1).tolist() + [label]\n",
    "    df_fft.loc[index] = fft_vector.reshape(-1).tolist() + [label]\n",
    "    df_gaussian.loc[index] = centroid.tolist() + gaussian.tolist() + [label]\n",
    "    df_pairwise.loc[index] = pairwise.reshape(-1).tolist() + [label]\n",
    "    df_shape_descriptors.loc[index] = centroid.tolist() + gaussian.tolist() + pairwise.reshape(-1).tolist() + [label]\n",
    "    \n",
    "\n",
    "df_scaled_rotated.to_csv(RECOGNITION_PATH + 'rotated_data_points.csv', index=False)\n",
    "df_fft.to_csv(RECOGNITION_PATH  + 'fft_data_points.csv', index=False)\n",
    "df_gaussian.to_csv(RECOGNITION_PATH  + 'gaussian_data_points.csv', index=False)\n",
    "df_pairwise.to_csv(RECOGNITION_PATH  + 'pairwise_data_points.csv', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset created with 3300 entries.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./dataset/anchor_points_dataset/rotated_data_points.csv')\n",
    "\n",
    "\n",
    "def Create_Verification_Dataset(data):\n",
    "    same_pairs = []\n",
    "    diff_pairs = []\n",
    "\n",
    "\n",
    "    while len(same_pairs) < 1650: # half of 3300\n",
    "        random_row_1 = df.sample(n=1)\n",
    "        same_label_rows = df[df['label'] == random_row_1['label'].iloc[0]]\n",
    "\n",
    "        if len(same_label_rows) > 1:\n",
    "            random_row_2 = same_label_rows.sample(n=1)\n",
    "            same_pairs.append((random_row_1.iloc[0].values[:-1], random_row_2.iloc[0].values[:-1], 1))\n",
    "\n",
    "    while len(diff_pairs) < 1650: # half of 3300\n",
    "        random_row_1 = df.sample(n=1)\n",
    "        diff_label_rows = df[df['label'] != random_row_1['label'].iloc[0]]\n",
    "\n",
    "        if len(diff_label_rows) > 1:\n",
    "            random_row_2 = diff_label_rows.sample(n=1)\n",
    "            diff_pairs.append((random_row_1.iloc[0].values[:-1], random_row_2.iloc[0].values[:-1], 0))\n",
    "\n",
    "    pairs = same_pairs + diff_pairs\n",
    "    random.shuffle(pairs)\n",
    "\n",
    "    data = []\n",
    "    for pair in pairs:\n",
    "        combined_features = pair[0].tolist() + pair[1].tolist()  \n",
    "        data.append(combined_features + [pair[2]])  \n",
    "\n",
    "    columns = [f'feature_{i}' for i in range(1, 2 * (len(df.columns) - 1) + 1)] + ['label']\n",
    "    return pd.DataFrame(data, columns=columns)\n",
    "\n",
    "Create_Verification_Dataset(df_scaled_rotated).to_csv(VALIDATION_PATH + 'rotated_data_points.csv', index=False)\n",
    "Create_Verification_Dataset(df_fft).to_csv(VALIDATION_PATH + 'fft_data_points.csv', index=False)\n",
    "Create_Verification_Dataset(df_gaussian).to_csv(VALIDATION_PATH  + 'gaussian_data_points.csv', index=False)\n",
    "Create_Verification_Dataset(df_pairwise).to_csv(VALIDATION_PATH + 'pairwise_data_points.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
