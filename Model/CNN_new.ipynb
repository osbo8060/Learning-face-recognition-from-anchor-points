{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality\n",
    "We would want to keep the embeddings of both the images and the feature vector to be about the same dimension. \n",
    "\n",
    "Current Features: \n",
    "\n",
    "Image: $255 \\times 255 \\times 3$\n",
    "\n",
    "AncorPoints: $204$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA ### \n",
    "trainSplit = 0.7\n",
    "testSplit = 0.15\n",
    "# (valSplit = 1 - trainSplit - testSplit)\n",
    "numImageChannels = 3 # 1 if grayscale or anchor points, 3 if colored images\n",
    "\n",
    "\n",
    "### CNN ARCHITECTURE ###\n",
    "# Define image input dimensions and kernel size\n",
    "inputImageOneDim = 128\n",
    "kernelSize = 3\n",
    "# Example: If our imput image is 128x128 pixels: inoutImageOneDim = 128\n",
    "\n",
    "# Dimensionality Reduction: Reduce dimensions drastically with Pooling Layers?\n",
    "poolingLayers = True\n",
    "poolingLayerKernelSize = 3\n",
    "\n",
    "### TRAINING ###\n",
    "initalLearningRate = 1e-3\n",
    "batchSize = 64\n",
    "numTrainEpochs = 30\n",
    "\n",
    "### LOSS FUNCTION ###\n",
    "# OBS! Select only one:\n",
    "\n",
    "lossFunction = nn.NLLLoss()\n",
    "# lossFunction = nn.CrossEntropyLoss() \n",
    "# lossFunction = nn.BCELoss()\n",
    "# triplet_Loss = True TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "useDevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(useDevice)\n",
    "print(\"Using Device: \" + useDevice)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),      \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the images\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=\"../Pre-processing/dataset/face_dataset/\", transform=transform)\n",
    "\n",
    "train_size = int(trainSplit * len(dataset))\n",
    "test_size = int(testSplit * len(dataset))\n",
    "val_size = len(dataset) - train_size - test_size\n",
    "train_dataset, test_dataset, val_dataset = random_split(dataset, [train_size, test_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# calculate steps per epoch for training and validation set\n",
    "trainSteps = len(train_loader.dataset) // batchSize\n",
    "valSteps = len(val_loader.dataset) // batchSize\n",
    "\n",
    "\n",
    "num_classes = len(dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Structure\n",
    "\n",
    "Trying to read up on our current experiment, I find it hard to find many sources or tips on how to construct a common architecture for two different inputs without changing the architecture between them. But let's begin with looking at the simple and most common image recognition architecture:\n",
    "\n",
    "\n",
    "From what I read, when doing facial recognition with images, we usually want multiple different convolutional layers, such as\n",
    "Conv Layer 1: To recognize edges\n",
    "Conv Layer 2: Recognize Corners and parts of the face\n",
    "Conv Layer 3: Recognize More concrete face features.\n",
    "\n",
    "And in between these have a pooling lyare to reduce dimensionality\n",
    "Thus the often proposed image recognition CNN is 3 Conv Layers, 3 pooling layers and then 1 or 2 fully connected layers.\n",
    "\n",
    "Experiments that are the closest to what we are doing that I found are multimodal models, let's take a look at how we might tackle this if we wanted to do a multi-modal model:\n",
    "\n",
    "What we could do is perform these steps on the images to reduce their dimensionality to a shared amount of nodes as the feature vectors and then have them share the final layers, such that we end up with something like the following architecture.\n",
    "\n",
    "$$\n",
    "\\begin{vmatrix}\n",
    "Image: 3x(conv -> pool)   \\\\\n",
    "Anchor: Linear -> Linear \n",
    "\\end{vmatrix}  -> Linear -> Linear/Output\n",
    "$$\n",
    "\n",
    "\n",
    "Thus we would share 2 layers in common with each other and it would be more of a multi modal model. Training two different models with these respective architectures would be to dissimilar however. We would end up with two completely different architectures:\n",
    "\n",
    "$$3(conv -> pool) -> 2Linear$$ \n",
    "and\n",
    "$$4 Linear$$\n",
    "\n",
    "So our options are to either \"give up\" on many of the layers in the classical image architecture, but if we abondon the general architecture which seems to be how image FR works best, are we really making a fair comparason? Or are we just unintentionally reducing the performance of our image recognition just to make a \"fair comparasson\"? \n",
    "\n",
    "My conclusion is thus that we have three reasonable ways to go forward. \n",
    "1. The easiest course of action would be to try to keep the architecture we know works for traditional Image Recognition, but try to represent our feature vector as more of an image and thus try to use the same/as similar as possible of an architecure for our Images as our Feature Vectors.\n",
    "2. Ignore the architecture problem and just try to create two models with as best performance/accuracy as possible but on the two different representations of the data. In this case, using a self-fitting CNN architecture as proposed in \"The Loss Surfaces of Multilayer Networks\" or similar could be a good idea.\n",
    "3. Just use different known CNN models and train them both on the data.\n",
    "\n",
    "Bellow I'll focus on 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 1\n",
      "2: 4\n",
      "3: 9\n",
      "4: 16\n",
      "5: 25\n",
      "6: 36\n",
      "7: 49\n",
      "8: 64\n",
      "9: 81\n",
      "10: 100\n",
      "11: 121\n",
      "12: 144\n",
      "13: 169\n",
      "14: 196\n",
      "15: 225\n"
     ]
    }
   ],
   "source": [
    "### Given our number of features, how many dims do we need to represent our features as an image?\n",
    "\n",
    "numberOfFeatures = 204\n",
    "counter = 1\n",
    "while True:\n",
    "    calculation = counter*counter\n",
    "    print(str(counter) + \": \" + str(calculation))\n",
    "    if(calculation > numberOfFeatures):\n",
    "        break\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten\n",
    "\n",
    "# Class taken and modified from https://pyimagesearch.com/2021/07/19/pytorch-training-your-first-convolutional-neural-network-cnn/\n",
    "class CNN(Module):\n",
    "    def __init__(self, numChannels, classes, inputImageOneDim):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # initialize first set of CONV => RELU => POOL layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=numChannels, out_channels=32, kernel_size=kernelSize)\n",
    "        inputImageOneDim = inputImageOneDim-kernelSize+1\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        if poolingLayers:\n",
    "            self.maxpool1 = nn.MaxPool2d(kernel_size=3)\n",
    "            inputImageOneDim = int(inputImageOneDim/3)\n",
    "        \n",
    "        # initialize second set of CONV => RELU => POOL layers\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=kernelSize)\n",
    "        inputImageOneDim = inputImageOneDim-kernelSize+1\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        if poolingLayers:\n",
    "            self.maxpool2 = nn.MaxPool2d(kernel_size=3)\n",
    "            inputImageOneDim = int(inputImageOneDim/3)\n",
    "        \n",
    "        # initialize third set of CONV => RELU => POOL layers\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=kernelSize)\n",
    "        inputImageOneDim = inputImageOneDim-kernelSize+1\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        if poolingLayers:\n",
    "            self.maxpool3 = nn.MaxPool2d(kernel_size=3)\n",
    "            inputImageOneDim = int(inputImageOneDim/3)\n",
    "\n",
    "        finalDim = inputImageOneDim*inputImageOneDim\n",
    "        self.fc1 = Linear(in_features=123 * finalDim, out_features=finalDim)\n",
    "        self.relu4 = ReLU()\n",
    "\n",
    "        #Softmax\n",
    "        self.fc2 = Linear(in_features=finalDim, out_features=classes)\n",
    "        self.logSoftmax = LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers and pooling\n",
    "        if poolingLayers:\n",
    "            x = self.maxpool1(self.relu1(self.conv1(x)))\n",
    "            x = self.maxpool2(self.relu2(self.conv2(x)))\n",
    "            x = self.maxpool3(self.relu3(self.conv3(x)))\n",
    "        else:\n",
    "            x = self.relu1(self.conv1(x))\n",
    "            x = self.relu2(self.conv2(x))\n",
    "            x = self.relu3(self.conv3(x))\n",
    "            \n",
    "        # Flatten the output for fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten to [batch_size, finalDim]\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.relu4(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Apply LogSoftmax for final output\n",
    "        output = self.logSoftmax(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] initializing the CNN model...\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "print(\"[INFO] initializing the CNN model...\")\n",
    "model = CNN(\n",
    "\tnumChannels=numImageChannels,\n",
    "\tclasses=train_size,\n",
    "    inputImageOneDim=inputImageOneDim).to(device)\n",
    "# initialize our optimizer and loss function\n",
    "opt = Adam(model.parameters(), lr=initalLearningRate)\n",
    "\n",
    "# initialize a dictionary to store training history\n",
    "H = {\n",
    "\t\"train_loss\": [],\n",
    "\t\"train_acc\": [],\n",
    "\t\"val_loss\": [],\n",
    "\t\"val_acc\": []\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model Over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training the network...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x8192 and 1107x9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[144], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m (x, y) \u001b[38;5;241m=\u001b[39m (x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# perform a forward pass and calculate the training loss\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m lossFunction(pred, y)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# zero out the gradients, perform the backpropagation step, and update the weights\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\FaceTorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\FaceTorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[142], line 66\u001b[0m, in \u001b[0;36mCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     63\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten to [batch_size, finalDim]\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Fully connected layers\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu4(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     67\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Apply LogSoftmax for final output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\FaceTorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\FaceTorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\FaceTorch\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x8192 and 1107x9)"
     ]
    }
   ],
   "source": [
    "# measure how long training is going to take\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "\n",
    "for epoch in range(0, numTrainEpochs):\n",
    "\t# set the model in training mode\n",
    "\tmodel.train()\n",
    "\t# initialize the total training and validation loss\n",
    "\ttotalTrainLoss = 0\n",
    "\ttotalValLoss = 0\n",
    "\t# initialize the number of correct predictions in the training and validation step\n",
    "\ttrainCorrect = 0\n",
    "\tvalCorrect = 0\n",
    "\t# loop over the training set\n",
    "\tfor (x, y) in train_loader:\n",
    "\t\t# send the input to the device\n",
    "\t\t(x, y) = (x.to(device), y.to(device))\n",
    "\t\t# perform a forward pass and calculate the training loss\n",
    "\t\tpred = model(x)\n",
    "\t\tloss = lossFunction(pred, y)\n",
    "\t\t# zero out the gradients, perform the backpropagation step, and update the weights\n",
    "\t\topt.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\topt.step()\n",
    "\t\t# add the loss to the total training loss so far and calculate the number of correct predictions\n",
    "\t\ttotalTrainLoss += loss\n",
    "\t\ttrainCorrect += (pred.argmax(1) == y).type(\n",
    "\t\t\ttorch.float).sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Module [CNN] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m (x, y) \u001b[38;5;241m=\u001b[39m (x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# make the predictions and calculate the validation loss\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m totalValLoss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m lossFunction(pred, y)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# calculate the number of correct predictions\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\FaceTorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\FaceTorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\FaceTorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:394\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_unimplemented\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Define the computation performed at every call.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 394\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] is missing the required \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m function\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    396\u001b[0m     )\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Module [CNN] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "# switch off autograd for evaluation\n",
    "with torch.no_grad():\n",
    "    # set the model in evaluation mode\n",
    "    model.eval()\n",
    "    # loop over the validation set\n",
    "    for (x, y) in val_loader:\n",
    "        # send the input to the device\n",
    "        (x, y) = (x.to(device), y.to(device))\n",
    "        # make the predictions and calculate the validation loss\n",
    "        pred = model(x)\n",
    "        totalValLoss += lossFunction(pred, y)\n",
    "        # calculate the number of correct predictions\n",
    "        valCorrect += (pred.argmax(1) == y).type(\n",
    "            torch.float).sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics FROM HERE BELLOW NOT TESTED\n",
    "\n",
    "(At the moment just copied from the example code and variable changed to relevant for us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[117], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# calculate the average training and validation loss\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m avgTrainLoss \u001b[38;5;241m=\u001b[39m \u001b[43mtotalTrainLoss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrainSteps\u001b[49m\n\u001b[0;32m      3\u001b[0m avgValLoss \u001b[38;5;241m=\u001b[39m totalValLoss \u001b[38;5;241m/\u001b[39m valSteps\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# calculate the training and validation accuracy\u001b[39;00m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# calculate the average training and validation loss\n",
    "avgTrainLoss = totalTrainLoss / trainSteps\n",
    "avgValLoss = totalValLoss / valSteps\n",
    "# calculate the training and validation accuracy\n",
    "trainCorrect = trainCorrect / len(train_loader.dataset)\n",
    "valCorrect = valCorrect / len(val_loader.dataset)\n",
    "# update our training history\n",
    "H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "H[\"train_acc\"].append(trainCorrect)\n",
    "H[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "H[\"val_acc\"].append(valCorrect)\n",
    "# print the model training and validation information\n",
    "print(\"[INFO] EPOCH: {}/{}\".format(e + 1, numTrainEpochs))\n",
    "print(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(\n",
    "    avgTrainLoss, trainCorrect))\n",
    "print(\"Val loss: {:.6f}, Val accuracy: {:.4f}\\n\".format(\n",
    "    avgValLoss, valCorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# finish measuring how long training took\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
    "\tendTime - startTime))\n",
    "# we can now evaluate the network on the test set\n",
    "print(\"[INFO] evaluating network...\")\n",
    "# turn off autograd for testing evaluation\n",
    "with torch.no_grad():\n",
    "\t# set the model in evaluation mode\n",
    "\tmodel.eval()\n",
    "\t\n",
    "\t# initialize a list to store our predictions\n",
    "\tpreds = []\n",
    "\t# loop over the test set\n",
    "\tfor (x, y) in test_loader:\n",
    "\t\t# send the input to the device\n",
    "\t\tx = x.to(device)\n",
    "\t\t# make the predictions and add them to the list\n",
    "\t\tpred = model(x)\n",
    "\t\tpreds.extend(pred.argmax(axis=1).cpu().numpy())\n",
    "# generate a classification report\n",
    "print(classification_report(test_loader.targets.cpu().numpy(),\n",
    "\tnp.array(preds), target_names=test_loader.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(H[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(H[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(H[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "# plt.savefig(args[\"plot\"])\n",
    "# serialize the model to disk\n",
    "# torch.save(model, args[\"model\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FaceTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
